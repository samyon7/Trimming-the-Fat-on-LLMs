{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True, num_token_types=3):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_token_types = num_token_types  # Number of token types (e.g., Global, Local, Sliding Window)\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # NAtS-related layers\n",
        "        self.token_type_scorer = nn.Linear(embed_dim, num_token_types) # Predicts token type\n",
        "        self.softmax = nn.Softmax(dim=-1)  # Convert scores to probabilities\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None):\n",
        "        seq_len, batch_size, embed_dim = query.size()\n",
        "\n",
        "        # Project queries, keys, and values\n",
        "        q = self.q_proj(query)\n",
        "        k = self.k_proj(key)\n",
        "        v = self.v_proj(value)\n",
        "\n",
        "        # Reshape to split into multiple heads\n",
        "        q = q.view(seq_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        k = k.view(seq_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        v = v.view(seq_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        # NAtS: Determine token types dynamically\n",
        "        token_type_scores = self.token_type_scorer(query)  # (seq_len, batch_size, num_token_types)\n",
        "        token_type_probs = self.softmax(token_type_scores) # (seq_len, batch_size, num_token_types)\n",
        "\n",
        "        # Construct dynamic attention mask based on token types\n",
        "        nats_attn_mask = self.construct_nats_attn_mask(token_type_probs, seq_len, batch_size) #  (batch_size * num_heads, seq_len, seq_len)\n",
        "\n",
        "        # Combine with provided attn_mask (if any)\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask + nats_attn_mask\n",
        "        else:\n",
        "            attn_mask = nats_attn_mask\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output, attn_weights = self.scaled_dot_product_attention(q, k, v, key_padding_mask, attn_mask)\n",
        "\n",
        "        # Concatenate heads and project output\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(seq_len, batch_size, embed_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def construct_nats_attn_mask(self, token_type_probs, seq_len, batch_size):\n",
        "         # token_type_probs shape: (seq_len, batch_size, num_token_types)\n",
        "        # num_token_types assumed to be: 0=Global, 1=Local, 2=Sliding Window\n",
        "\n",
        "        nats_attn_mask = torch.zeros((batch_size * self.num_heads, seq_len, seq_len), device=token_type_probs.device)\n",
        "\n",
        "        # Create masks for each token type\n",
        "        global_mask = (token_type_probs[:, :, 0] > 0.5).float()  # Example threshold (you might need to adjust)\n",
        "        local_mask = (token_type_probs[:, :, 1] > 0.5).float()\n",
        "        sliding_window_mask = (token_type_probs[:, :, 2] > 0.5).float()\n",
        "\n",
        "        # Iterate through each sequence position\n",
        "        for i in range(seq_len):\n",
        "            for j in range(seq_len):\n",
        "                # Global Token: Can attend to all previous tokens\n",
        "                if global_mask[i].any():\n",
        "                    nats_attn_mask[:, i, j] = 0.0  # No mask (attend)\n",
        "\n",
        "                # Local Token: Can attend to only limited tokens\n",
        "                elif local_mask[i].any() and i > j :\n",
        "                    nats_attn_mask[:, i, j] = float('-inf')  # Apply mask (do not attend)\n",
        "\n",
        "                # Sliding Window Token: Apply a sliding window\n",
        "                elif sliding_window_mask[i].any():  # Fixed sliding window size of 3 (adjust as needed)\n",
        "                     if i - j > 3 or i < j:\n",
        "                         nats_attn_mask[:, i, j] = float('-inf')  # Apply mask (do not attend)\n",
        "        return nats_attn_mask\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, key_padding_mask=None, attn_mask=None):\n",
        "        d_k = q.size(-1)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            scores = scores + attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        return attn_output, attn_weights"
      ],
      "metadata": {
        "id": "SDrCc05mCDRD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_token_types = 3\n",
        "seq_len = 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create a sample MultiheadAttention module:\n",
        "mha = MultiheadAttention(embed_dim, num_heads, num_token_types=num_token_types)\n",
        "\n",
        "# Create random query, key, and value tensors:\n",
        "query = torch.randn(seq_len, batch_size, embed_dim)\n",
        "key = torch.randn(seq_len, batch_size, embed_dim)\n",
        "value = torch.randn(seq_len, batch_size, embed_dim)\n",
        "\n",
        "# Run the forward pass:\n",
        "attn_output, attn_weights = mha(query, key, value)\n",
        "\n",
        "# Print the output shapes:\n",
        "print(\"Attention Output Shape:\", attn_output.shape)\n",
        "print(\"Attention Weights Shape:\", attn_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWG9QlvcGAMN",
        "outputId": "75002331-473b-467b-b753-f8421bbc836e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output Shape: torch.Size([32, 16, 512])\n",
            "Attention Weights Shape: torch.Size([128, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r09c_E51GCpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}