# Trimming-the-Fat-on-LLMs

# ğŸ”¥ **Dynamic Attention Done Right**  

Welcome to the **MultiheadAttention** class â€” a straight-up fire implementation of multihead self-attention with a *next-level twist*: **NAtS (Neural Attention Token Selector)**.  
This beast dynamically classifies tokens into **Global**, **Local**, and **Sliding Window** categories and builds a custom attention mask on the fly.  

Letâ€™s break this down mathematically and keep it ğŸ’¯ academic yet chill.  

---

## ğŸ’¡ **Key Concepts**  

### ğŸ”¥ Multihead Self-Attention (MHA)  
We got the OG transformer vibes here â€” splitting the embedding into `num_heads` partitions. For each head:  

$$
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
$$  

where \(X \in \mathbb{R}^{L \times B \times d}\) (seq_len, batch_size, embed_dim).  
Heads run in parallel because we like that speed â©.  

---

### ğŸ§  **Scaled Dot-Product Attention**  
We compute:  

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
$$  

with \(M\) as the attention mask. The scaling by \(\sqrt{d_k}\) prevents that â€œtoo spicyâ€ softmax explosion. ğŸŒ¶ï¸  

---

## ğŸš€ **But Wait... We Got NAtS!**  
This ainâ€™t your regular attention layer. We dynamically score each token type:  

$$
P(\text{token\_type}) = \text{softmax}(XW^{\text{type}})
$$  

where \(W^{\text{type}}\) maps embeddings into a \(\mathbb{R}^{d \times 3}\) space â€” representing our holy trinity:  
1. **Global** (Yo, I attend to everything) ğŸŒ  
2. **Local** (Just vibinâ€™ with the neighborhood) ğŸ¡  
3. **Sliding Window** (Rollin' through time, one frame at a time) ğŸï¸  

---

## ğŸ—ï¸ **Dynamic Mask Construction**  
Based on token probs, we build the attention mask \(M\):  

- **Global Tokens**:  
  $$
  M_{i,j} = 0 \quad \forall j \leq i \quad \text{(full context access)}  
  $$  

- **Local Tokens**:  
  $$
  M_{i,j} = -\infty \quad \text{if } j < i \quad \text{(no peeking too far back)}  
  $$  

- **Sliding Window Tokens (window=3)**:  
  $$
  M_{i,j} = 
  \begin{cases} 
  0 & \text{if } |i-j| \leq 3 \\ 
  -\infty & \text{otherwise} 
  \end{cases}
  $$  

This mask keeps the attention focused, efficient, and context-aware, like a well-tuned Markov chain ğŸ”—.  

---

## ğŸ›ï¸ **Code Walkthrough**  

### `__init__()` â€” *Setting up the party* ğŸ‰  
- Splits embeddings into heads (`embed_dim // num_heads`)  
- Sets up linear projections for Q, K, V  
- Adds a token-type scorer (NAtS brain)  
- Drops out (literally) for regularization  

---

### `forward()` â€” *Where the magic happens* âœ¨  
1. **Projection**: Compute Q, K, V from inputs.  
2. **Reshaping**: Turn \(B, L, D\) into \(B \times H, L, d_h\).  
3. **Token Typing**: Predict token types dynamically:  

   $$
   P(t) = \text{softmax}(XW^{\text{type}})
   $$  

4. **Dynamic Masking**: Generate attention mask via `construct_nats_attn_mask()`.  
5. **Scaled Dot-Product**: Compute attention and apply dropout.  
6. **Output Projection**: Concatenate heads and project back.  

---

### `construct_nats_attn_mask()` â€” *NAtS in action* ğŸ§©  
This function loops through each token and dynamically assigns who it can attend to based on learned probabilities. Think of it like a bouncer letting only certain tokens into the VIP section. ğŸŸï¸  

---

### `scaled_dot_product_attention()` â€” *Classic transformer math* ğŸ§®  
Executes the OG attention formula:  

$$
\alpha_{ij} = \frac{\exp\left(\frac{(QK^T)_{ij}}{\sqrt{d_k}}\right)}{\sum_j \exp\left(\frac{(QK^T)_{ij}}{\sqrt{d_k}}\right)}
$$  

Multiplies by \(V\) to get the final output:  

$$
\text{Output} = \alpha V
$$  

---

## ğŸƒ **Why This Rocks**  
- ğŸ§  **Dynamic Attention**: Adaptive masking = smarter token dependencies.  
- ğŸš€ **Efficient Context Use**: Focuses compute where it matters.  
- ğŸ”¥ **Flexible Architecture**: Just tweak the number of token types or thresholds.  

---

## ğŸ¯ **TL;DR**  
This code implements a multihead attention mechanism that doesnâ€™t just blindly attend everywhere. Instead, it **learns** what matters, dynamically shifting between global, local, and sliding-window attention modes.  

Special credits for my brother and my friend, Jo. Dani and Wanto

---
